{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b4daaa-6ff8-467c-8a9b-c56d3e4d9f8b",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = https://project.lsst.org/sites/default/files/Rubin-O-Logo_0.png width=250 style=\"padding: 10px\" alt=\"Vera C. Rubin Observatory Logo\"> \n",
    "<h1 style=\"margin-top: 10px\">Retrieve and Aggregate Zooniverse Output</h1>\n",
    "Authors: Becky Nevin, Clare Higgs, and Eric Rosas <br>\n",
    "Contact author: Clare Higgs <br>\n",
    "Last verified to run: 2024-06-20 <br>\n",
    "LSST Science Pipelines version: Weekly 2024_04 <br>\n",
    "Container size: small or medium <br>\n",
    "Targeted learning level: intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16d57a-9af8-4fc9-bf86-8724024516e6",
   "metadata": {},
   "source": [
    "<b>Description:</b> This notebook guides a PI through the process of retrieving classification data from Zooniverse and builds upon Hayley Robert's Aggregation notebook example. <br><br>\n",
    "<b>Skills:</b> \n",
    "<br><br>\n",
    "<b>LSST Data Products:</b> n/a<br><br>\n",
    "<b>Packages:</b> rubin.citsci, utils (citsci plotting and display utilities),  <br><br>\n",
    "<b>Credit:</b> Hayley Roberts<br><br>\n",
    "<b>Get Support: </b>PIs new to DP0 are encouraged to find documentation and resources at <a href=\"https://dp0-2.lsst.io/\">dp0-2.lsst.io</a>. Support for this notebook is available and questions are welcome at cscience@lsst.org."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa844a-2bd3-41e4-9058-f5ce85c3fc54",
   "metadata": {},
   "source": [
    "## 1. Introduction <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "This notebook provides an introduction to how to use the ???? and rubin.citsci package to retrieve classifications from Zooniverse and aggregate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3b241-4e92-49cd-9148-c6b180dc11d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T17:44:28.705146Z",
     "iopub.status.busy": "2024-06-20T17:44:28.704785Z",
     "iopub.status.idle": "2024-06-20T17:44:28.708543Z",
     "shell.execute_reply": "2024-06-20T17:44:28.707909Z",
     "shell.execute_reply.started": "2024-06-20T17:44:28.705123Z"
    }
   },
   "source": [
    "### 1.1 Package imports <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cdc872-8bdf-485b-921f-bb9bd5b597dd",
   "metadata": {},
   "source": [
    "#### Install Pipeline Package\n",
    "\n",
    "First, install the Rubin Citizen Science Pipeline package by doing the following:\n",
    "\n",
    "1. Open up a New Launcher tab\n",
    "2. In the \"Other\" section of the New Launcher tab, click \"Terminal\"\n",
    "3. Use `pip` to install the `rubin.citsci` package by entering the following command:\n",
    "```\n",
    "pip install rubin.citsci\n",
    "```\n",
    "Note that this package will soon be installed directly on RSP.\n",
    "\n",
    "If this package is already installed, make sure it is updated:\n",
    "```\n",
    "pip install --u rubin.citsci\n",
    "```\n",
    "\n",
    "4. Confirm the next cell containing `from rubin.citsci import pipeline` works as expected and does not throw an error\n",
    "\n",
    "5. Install panoptes_client:\n",
    "```\n",
    "pip install panoptes_client\n",
    "pip install panoptes_aggregation\n",
    "```\n",
    "\n",
    "6. If the pip install doesn't work for `panoptes_aggregation`:\n",
    "```\n",
    "pip install -U git+git://github.com/zooniverse/aggregation-for-caesar.git\n",
    "```\n",
    "(https://www.zooniverse.org/talk/1322/2415041?comment=3969837&page=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e566b7c-4fb3-4195-b68a-39e4dc1a374c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T15:59:18.552714Z",
     "iopub.status.busy": "2024-06-26T15:59:18.552037Z",
     "iopub.status.idle": "2024-06-26T15:59:18.555662Z",
     "shell.execute_reply": "2024-06-26T15:59:18.555044Z",
     "shell.execute_reply.started": "2024-06-26T15:59:18.552685Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is all from SLSN_batch_aggregation.py\n",
    "# found here - \n",
    "# https://github.com/astrohayley/SLSN-Aggregation-Example/blob/main/SLSN_batch_aggregation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d65d903-8012-43e3-ab19-d94b3f422e83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:21:53.348628Z",
     "iopub.status.busy": "2024-06-26T17:21:53.347925Z",
     "iopub.status.idle": "2024-06-26T17:21:53.351769Z",
     "shell.execute_reply": "2024-06-26T17:21:53.351294Z",
     "shell.execute_reply.started": "2024-06-26T17:21:53.348601Z"
    }
   },
   "outputs": [],
   "source": [
    "# basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Zooniverse tools\n",
    "from panoptes_client import Panoptes, Workflow\n",
    "from panoptes_aggregation.extractors.utilities import annotation_by_task\n",
    "from panoptes_aggregation.extractors import question_extractor\n",
    "from panoptes_aggregation.reducers import question_consensus_reducer\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d951495-9ed7-44ed-bffe-04504679e8bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:21:58.424257Z",
     "iopub.status.busy": "2024-06-26T17:21:58.423978Z",
     "iopub.status.idle": "2024-06-26T17:21:58.435053Z",
     "shell.execute_reply": "2024-06-26T17:21:58.434558Z",
     "shell.execute_reply.started": "2024-06-26T17:21:58.424237Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_classifications(WORKFLOW_ID, client):\n",
    "    \"\"\"\n",
    "    Downloads data from Zooniverse\n",
    "\n",
    "    Args:\n",
    "        WORKFLOW_ID (int): Workflow ID of workflow being aggregated\n",
    "        client: Logged in Zooniverse client\n",
    "\n",
    "    Returns:\n",
    "        classification_data (DataFrame): Raw classifications from Zooniverse\n",
    "    \"\"\"\n",
    "\n",
    "    workflow = Workflow(WORKFLOW_ID)\n",
    "\n",
    "    # generate the classifications \n",
    "    with client:\n",
    "        classification_export = workflow.get_export('classifications', generate=True, wait=True)\n",
    "        classification_rows = [row for row in tqdm(classification_export.csv_dictreader())]\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    classification_data = pd.DataFrame.from_dict(classification_rows)\n",
    "\n",
    "    return classification_data\n",
    "\n",
    "\n",
    "\n",
    "def extract_data(classification_data):\n",
    "    \"\"\"\n",
    "    Extracts annotations from the classification data\n",
    "\n",
    "    Args:\n",
    "        classification_data (DataFrame): Raw classifications from Zooniverse\n",
    "\n",
    "    Returns:\n",
    "        extracted_data (DataFrame): Extracted annotations from raw classification data\n",
    "    \"\"\"\n",
    "    # set up our list where we will store the extracted data temporarily\n",
    "    extracted_rows = []\n",
    "\n",
    "    # iterate through our classification data\n",
    "    for i in range(len(classification_data)):\n",
    "\n",
    "        # access the specific row and extract the annotations\n",
    "        row = classification_data.iloc[i]\n",
    "        for annotation in json.loads(row.annotations):\n",
    "\n",
    "            row_annotation = annotation_by_task({'annotations': [annotation]})\n",
    "            extract = question_extractor(row_annotation)\n",
    "\n",
    "            # add the extracted annotations to our temporary list along with some other additional data\n",
    "            extracted_rows.append({\n",
    "                'classification_id': row.classification_id,\n",
    "                'subject_id':        row.subject_ids,\n",
    "                'user_name':         row.user_name,\n",
    "                'user_id':           row.user_id,\n",
    "                'created_at':        row.created_at,\n",
    "                'data':              json.dumps(extract),\n",
    "                'task':              annotation['task']\n",
    "            })\n",
    "\n",
    "\n",
    "    # convert the extracted data to a pandas dataframe and sort\n",
    "    extracted_data = pd.DataFrame.from_dict(extracted_rows)\n",
    "    extracted_data.sort_values(['subject_id', 'created_at'], inplace=True)\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "\n",
    "def last_filter(data):\n",
    "    \"\"\"\n",
    "    Determines the most recently submitted classifications\n",
    "    \"\"\"\n",
    "    last_time = data.created_at.max()\n",
    "    ldx = data.created_at == last_time\n",
    "    return data[ldx]\n",
    "\n",
    "\n",
    "def aggregate_data(extracted_data):\n",
    "    \"\"\"\n",
    "    Aggregates question data from extracted annotations\n",
    "\n",
    "    Args:\n",
    "        extracted_data (DataFrame): Extracted annotations from raw classifications\n",
    "\n",
    "    Returns:\n",
    "        aggregated_data (DataFrame): Aggregated data for the given workflow\n",
    "    \"\"\"\n",
    "    # generate an array of unique subject ids - these are the ones that we will iterate over\n",
    "    subject_ids_unique = np.unique(extracted_data.subject_id)\n",
    "\n",
    "    # set up a temporary list to store reduced data\n",
    "    aggregated_rows = []\n",
    "\n",
    "    # determine the total number of tasks\n",
    "    tasks = np.unique(extracted_data.task)\n",
    "\n",
    "    # iterating over each unique subject id\n",
    "    for i in range(len(subject_ids_unique)):\n",
    "\n",
    "        # determine the subject_id to work on\n",
    "        subject_id = subject_ids_unique[i]\n",
    "\n",
    "        # filter the extract_data dataframe for only the subject_id that is being worked on\n",
    "        extract_data_subject = extracted_data[extracted_data.subject_id==subject_id].drop_duplicates()\n",
    "\n",
    "        for task in tasks:\n",
    "\n",
    "            extract_data_filtered = extract_data_subject[extract_data_subject.task == task]\n",
    "\n",
    "            # if there are less unique user submissions than classifications, filter for the most recently updated classification\n",
    "            if (len(extract_data_filtered.user_name.unique()) < len(extract_data_filtered)):\n",
    "                extract_data_filtered = extract_data_filtered.groupby(['user_name'], group_keys=False).apply(last_filter)\n",
    "\n",
    "            # iterate through the filtered extract data to prepare for the reducer\n",
    "            classifications_to_reduce = [json.loads(extract_data_filtered.iloc[j].data) for j in range(len(extract_data_filtered))]\n",
    "\n",
    "            # use the Zooniverse question_consesus_reducer to get the final consensus\n",
    "            reduction = question_consensus_reducer(classifications_to_reduce)\n",
    "\n",
    "            # add the subject id to our reduction data\n",
    "            reduction['subject_id'] = subject_id\n",
    "            reduction['task'] = task\n",
    "\n",
    "            # add the data to our temporary list\n",
    "            aggregated_rows.append(reduction)\n",
    "\n",
    "\n",
    "    # converting the result to a dataframe\n",
    "    aggregated_data = pd.DataFrame.from_dict(aggregated_rows)\n",
    "\n",
    "    # drop rows that are nan\n",
    "    aggregated_data.dropna(inplace=True)\n",
    "\n",
    "    return aggregated_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_aggregation(generate_new_classifications=False, WORKFLOW_ID=13193): \n",
    "    \"\"\"\n",
    "    Downloads raw classifications, extracts annotations, and aggregates data\n",
    "\n",
    "    Args:\n",
    "        WORKFLOW_ID (int): Workflow ID of workflow being aggregated\n",
    "        client: Logged in Zooniverse client\n",
    "\n",
    "    Returns:\n",
    "        aggregated_data (DataFrame): Aggregated data for the given workflow\n",
    "    \"\"\"\n",
    "\n",
    "    if generate_new_classifications:\n",
    "        # connect to client and download data\n",
    "        print('Sign in to zooniverse.org:')\n",
    "        client = Panoptes.client(username=getpass.getpass('username: '), password=getpass.getpass('password: '))\n",
    "        print('Generating classification data - could take some time')\n",
    "        classification_data = get_data_from_zooniverse(WORKFLOW_ID=WORKFLOW_ID, client=client)\n",
    "        print('Saving classifications')\n",
    "        classification_data.to_csv('superluminous-supernovae-classifications.csv', index=False)\n",
    "    else:\n",
    "        # or just open the file\n",
    "        print('Loading classifications')\n",
    "        classification_data = pd.read_csv('superluminous-supernovae-classifications.csv')\n",
    "\n",
    "    # limit classifications to those for the relevant workflow\n",
    "    classification_data = classification_data[classification_data.workflow_id==WORKFLOW_ID]\n",
    "\n",
    "    # extract annotations\n",
    "    print('Extracting annotations')\n",
    "    extracted_data = extract_data(classification_data=classification_data)\n",
    "\n",
    "    # aggregate annotations\n",
    "    print('Aggregating data')\n",
    "    final_data = aggregate_data(extracted_data=extracted_data)\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc18305d-acb6-4bc7-9f39-74feb1e7d197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:19:47.071064Z",
     "iopub.status.busy": "2024-06-26T17:19:47.070751Z",
     "iopub.status.idle": "2024-06-26T17:19:47.516588Z",
     "shell.execute_reply": "2024-06-26T17:19:47.515995Z",
     "shell.execute_reply.started": "2024-06-26T17:19:47.071043Z"
    }
   },
   "outputs": [],
   "source": [
    "from rubin.citsci import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadd85aa-7590-460f-800d-873814d6e50c",
   "metadata": {
    "execution": {
     "execution_failed": "2024-06-26T17:22:05.406Z",
     "iopub.execute_input": "2024-06-26T17:22:00.225613Z",
     "iopub.status.busy": "2024-06-26T17:22:00.224837Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and running utilities to establish a link with Zooniverse\n",
      "Enter your Zooniverse username followed by password below\n",
      "WARNING! : You currently have v0.4.3 of this package installed, but v0.5.0 is available.\n",
      "To install the latest version, open up a terminal tab and run the following command:\n",
      "    pip install --upgrade --force-reinstall rubin.citsci\n",
      "After the upgrade installation has finished, please restart the kernel for the changes to take effect.\n",
      "Enter your Zooniverse credentials...\n"
     ]
    }
   ],
   "source": [
    "email = \"beckynevin@gmail.com\"\n",
    "slug_name = \"rebecca-dot-nevin/test-project\"\n",
    "print(\"Loading and running utilities to establish a link with Zooniverse\")\n",
    "print(\"Enter your Zooniverse username followed by password below\")\n",
    "cit_sci_pipeline = pipeline.CitSciPipeline()\n",
    "cit_sci_pipeline.login_to_zooniverse(slug_name, email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e0291a-d5e7-4fca-88d0-6930d32f2d4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-26T17:21:34.529530Z",
     "iopub.status.busy": "2024-06-26T17:21:34.528860Z",
     "iopub.status.idle": "2024-06-26T17:21:37.110334Z",
     "shell.execute_reply": "2024-06-26T17:21:37.109437Z",
     "shell.execute_reply.started": "2024-06-26T17:21:34.529501Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m WORKFLOW_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m23254\u001b[39m\n\u001b[1;32m      3\u001b[0m client \u001b[38;5;241m=\u001b[39m cit_sci_pipeline\u001b[38;5;241m.\u001b[39mclient\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdownload_classifications\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWORKFLOW_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m, in \u001b[0;36mdownload_classifications\u001b[0;34m(WORKFLOW_ID, client)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m client:\n\u001b[1;32m     17\u001b[0m     classification_export \u001b[38;5;241m=\u001b[39m workflow\u001b[38;5;241m.\u001b[39mget_export(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifications\u001b[39m\u001b[38;5;124m'\u001b[39m, generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m     classification_rows \u001b[38;5;241m=\u001b[39m [row \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(classification_export\u001b[38;5;241m.\u001b[39mcsv_dictreader())]\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# convert to pandas dataframe\u001b[39;00m\n\u001b[1;32m     21\u001b[0m classification_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(classification_rows)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "#project_id = 19539\n",
    "WORKFLOW_ID = 23254\n",
    "client = cit_sci_pipeline.client\n",
    "download_classifications(WORKFLOW_ID, client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8cd49-3975-4dec-accd-ef853385bc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
