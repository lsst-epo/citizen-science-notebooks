{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b4daaa-6ff8-467c-8a9b-c56d3e4d9f8b",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = https://project.lsst.org/sites/default/files/Rubin-O-Logo_0.png width=250 style=\"padding: 10px\" alt=\"Vera C. Rubin Observatory Logo\"> \n",
    "<h1 style=\"margin-top: 10px\">Retrieve and Aggregate Zooniverse Output</h1>\n",
    "Authors: Becky Nevin, Clare Higgs, and Eric Rosas <br>\n",
    "Contact author: Clare Higgs <br>\n",
    "Last verified to run: 2024-06-20 <br>\n",
    "LSST Science Pipelines version: Weekly 2024_04 <br>\n",
    "Container size: small or medium <br>\n",
    "Targeted learning level: intermediate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16d57a-9af8-4fc9-bf86-8724024516e6",
   "metadata": {},
   "source": [
    "<b>Description:</b> This notebook guides a PI through the process of retrieving classification data from Zooniverse and builds upon Hayley Robert's Aggregation notebook example. <br><br>\n",
    "<b>Skills:</b> \n",
    "<br><br>\n",
    "<b>LSST Data Products:</b> n/a<br><br>\n",
    "<b>Packages:</b> rubin.citsci, utils (citsci plotting and display utilities),  <br><br>\n",
    "<b>Credit:</b> Hayley Roberts<br><br>\n",
    "<b>Get Support: </b>PIs new to DP0 are encouraged to find documentation and resources at <a href=\"https://dp0-2.lsst.io/\">dp0-2.lsst.io</a>. Support for this notebook is available and questions are welcome at cscience@lsst.org."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faa844a-2bd3-41e4-9058-f5ce85c3fc54",
   "metadata": {},
   "source": [
    "## 1. Introduction <a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "This notebook provides an introduction to how to use the ???? and rubin.citsci package to retrieve classifications from Zooniverse and aggregate the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de3b241-4e92-49cd-9148-c6b180dc11d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T17:44:28.705146Z",
     "iopub.status.busy": "2024-06-20T17:44:28.704785Z",
     "iopub.status.idle": "2024-06-20T17:44:28.708543Z",
     "shell.execute_reply": "2024-06-20T17:44:28.707909Z",
     "shell.execute_reply.started": "2024-06-20T17:44:28.705123Z"
    }
   },
   "source": [
    "### 1.1 Package imports <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cdc872-8bdf-485b-921f-bb9bd5b597dd",
   "metadata": {},
   "source": [
    "#### Install Pipeline Package\n",
    "\n",
    "First, install the Rubin Citizen Science Pipeline package by doing the following:\n",
    "\n",
    "1. Open up a New Launcher tab\n",
    "2. In the \"Other\" section of the New Launcher tab, click \"Terminal\"\n",
    "3. Use `pip` to install the `rubin.citsci` package by entering the following command:\n",
    "```\n",
    "pip install rubin.citsci\n",
    "```\n",
    "Note that this package will soon be installed directly on RSP.\n",
    "\n",
    "If this package is already installed, make sure it is updated:\n",
    "```\n",
    "pip install --u rubin.citsci\n",
    "```\n",
    "\n",
    "4. Confirm the next cell containing `from rubin.citsci import pipeline` works as expected and does not throw an error\n",
    "\n",
    "5. Install panoptes_client:\n",
    "```\n",
    "pip install panoptes_client\n",
    "pip install panoptes_aggregation\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e566b7c-4fb3-4195-b68a-39e4dc1a374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is all from SLSN_batch_aggregation.py\n",
    "# found here - \n",
    "# https://github.com/astrohayley/SLSN-Aggregation-Example/blob/main/SLSN_batch_aggregation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d951495-9ed7-44ed-bffe-04504679e8bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T17:58:18.462209Z",
     "iopub.status.busy": "2024-06-20T17:58:18.461997Z",
     "iopub.status.idle": "2024-06-20T17:58:19.960672Z",
     "shell.execute_reply": "2024-06-20T17:58:19.959847Z",
     "shell.execute_reply.started": "2024-06-20T17:58:18.462195Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'panoptes_aggregation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Zooniverse tools\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpanoptes_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Panoptes, Workflow\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpanoptes_aggregation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextractors\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotation_by_task\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpanoptes_aggregation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextractors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_extractor\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpanoptes_aggregation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreducers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m question_consensus_reducer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'panoptes_aggregation'"
     ]
    }
   ],
   "source": [
    "# basics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import getpass\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Zooniverse tools\n",
    "from panoptes_client import Panoptes, Workflow\n",
    "from panoptes_aggregation.extractors.utilities import annotation_by_task\n",
    "from panoptes_aggregation.extractors import question_extractor\n",
    "from panoptes_aggregation.reducers import question_consensus_reducer\n",
    "\n",
    "\n",
    "\n",
    "def download_classifications(WORKFLOW_ID, client):\n",
    "    \"\"\"\n",
    "    Downloads data from Zooniverse\n",
    "\n",
    "    Args:\n",
    "        WORKFLOW_ID (int): Workflow ID of workflow being aggregated\n",
    "        client: Logged in Zooniverse client\n",
    "\n",
    "    Returns:\n",
    "        classification_data (DataFrame): Raw classifications from Zooniverse\n",
    "    \"\"\"\n",
    "\n",
    "    workflow = Workflow(WORKFLOW_ID)\n",
    "\n",
    "    # generate the classifications \n",
    "    with client:\n",
    "        classification_export = workflow.get_export('classifications', generate=True, wait=True)\n",
    "        classification_rows = [row for row in tqdm(classification_export.csv_dictreader())]\n",
    "\n",
    "    # convert to pandas dataframe\n",
    "    classification_data = pd.DataFrame.from_dict(classification_rows)\n",
    "\n",
    "    return classification_data\n",
    "\n",
    "\n",
    "\n",
    "def extract_data(classification_data):\n",
    "    \"\"\"\n",
    "    Extracts annotations from the classification data\n",
    "\n",
    "    Args:\n",
    "        classification_data (DataFrame): Raw classifications from Zooniverse\n",
    "\n",
    "    Returns:\n",
    "        extracted_data (DataFrame): Extracted annotations from raw classification data\n",
    "    \"\"\"\n",
    "    # set up our list where we will store the extracted data temporarily\n",
    "    extracted_rows = []\n",
    "\n",
    "    # iterate through our classification data\n",
    "    for i in range(len(classification_data)):\n",
    "\n",
    "        # access the specific row and extract the annotations\n",
    "        row = classification_data.iloc[i]\n",
    "        for annotation in json.loads(row.annotations):\n",
    "\n",
    "            row_annotation = annotation_by_task({'annotations': [annotation]})\n",
    "            extract = question_extractor(row_annotation)\n",
    "\n",
    "            # add the extracted annotations to our temporary list along with some other additional data\n",
    "            extracted_rows.append({\n",
    "                'classification_id': row.classification_id,\n",
    "                'subject_id':        row.subject_ids,\n",
    "                'user_name':         row.user_name,\n",
    "                'user_id':           row.user_id,\n",
    "                'created_at':        row.created_at,\n",
    "                'data':              json.dumps(extract),\n",
    "                'task':              annotation['task']\n",
    "            })\n",
    "\n",
    "\n",
    "    # convert the extracted data to a pandas dataframe and sort\n",
    "    extracted_data = pd.DataFrame.from_dict(extracted_rows)\n",
    "    extracted_data.sort_values(['subject_id', 'created_at'], inplace=True)\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "\n",
    "def last_filter(data):\n",
    "    \"\"\"\n",
    "    Determines the most recently submitted classifications\n",
    "    \"\"\"\n",
    "    last_time = data.created_at.max()\n",
    "    ldx = data.created_at == last_time\n",
    "    return data[ldx]\n",
    "\n",
    "\n",
    "def aggregate_data(extracted_data):\n",
    "    \"\"\"\n",
    "    Aggregates question data from extracted annotations\n",
    "\n",
    "    Args:\n",
    "        extracted_data (DataFrame): Extracted annotations from raw classifications\n",
    "\n",
    "    Returns:\n",
    "        aggregated_data (DataFrame): Aggregated data for the given workflow\n",
    "    \"\"\"\n",
    "    # generate an array of unique subject ids - these are the ones that we will iterate over\n",
    "    subject_ids_unique = np.unique(extracted_data.subject_id)\n",
    "\n",
    "    # set up a temporary list to store reduced data\n",
    "    aggregated_rows = []\n",
    "\n",
    "    # determine the total number of tasks\n",
    "    tasks = np.unique(extracted_data.task)\n",
    "\n",
    "    # iterating over each unique subject id\n",
    "    for i in range(len(subject_ids_unique)):\n",
    "\n",
    "        # determine the subject_id to work on\n",
    "        subject_id = subject_ids_unique[i]\n",
    "\n",
    "        # filter the extract_data dataframe for only the subject_id that is being worked on\n",
    "        extract_data_subject = extracted_data[extracted_data.subject_id==subject_id].drop_duplicates()\n",
    "\n",
    "        for task in tasks:\n",
    "\n",
    "            extract_data_filtered = extract_data_subject[extract_data_subject.task == task]\n",
    "\n",
    "            # if there are less unique user submissions than classifications, filter for the most recently updated classification\n",
    "            if (len(extract_data_filtered.user_name.unique()) < len(extract_data_filtered)):\n",
    "                extract_data_filtered = extract_data_filtered.groupby(['user_name'], group_keys=False).apply(last_filter)\n",
    "\n",
    "            # iterate through the filtered extract data to prepare for the reducer\n",
    "            classifications_to_reduce = [json.loads(extract_data_filtered.iloc[j].data) for j in range(len(extract_data_filtered))]\n",
    "\n",
    "            # use the Zooniverse question_consesus_reducer to get the final consensus\n",
    "            reduction = question_consensus_reducer(classifications_to_reduce)\n",
    "\n",
    "            # add the subject id to our reduction data\n",
    "            reduction['subject_id'] = subject_id\n",
    "            reduction['task'] = task\n",
    "\n",
    "            # add the data to our temporary list\n",
    "            aggregated_rows.append(reduction)\n",
    "\n",
    "\n",
    "    # converting the result to a dataframe\n",
    "    aggregated_data = pd.DataFrame.from_dict(aggregated_rows)\n",
    "\n",
    "    # drop rows that are nan\n",
    "    aggregated_data.dropna(inplace=True)\n",
    "\n",
    "    return aggregated_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch_aggregation(generate_new_classifications=False, WORKFLOW_ID=13193): \n",
    "    \"\"\"\n",
    "    Downloads raw classifications, extracts annotations, and aggregates data\n",
    "\n",
    "    Args:\n",
    "        WORKFLOW_ID (int): Workflow ID of workflow being aggregated\n",
    "        client: Logged in Zooniverse client\n",
    "\n",
    "    Returns:\n",
    "        aggregated_data (DataFrame): Aggregated data for the given workflow\n",
    "    \"\"\"\n",
    "\n",
    "    if generate_new_classifications:\n",
    "        # connect to client and download data\n",
    "        print('Sign in to zooniverse.org:')\n",
    "        client = Panoptes.client(username=getpass.getpass('username: '), password=getpass.getpass('password: '))\n",
    "        print('Generating classification data - could take some time')\n",
    "        classification_data = get_data_from_zooniverse(WORKFLOW_ID=WORKFLOW_ID, client=client)\n",
    "        print('Saving classifications')\n",
    "        classification_data.to_csv('superluminous-supernovae-classifications.csv', index=False)\n",
    "    else:\n",
    "        # or just open the file\n",
    "        print('Loading classifications')\n",
    "        classification_data = pd.read_csv('superluminous-supernovae-classifications.csv')\n",
    "\n",
    "    # limit classifications to those for the relevant workflow\n",
    "    classification_data = classification_data[classification_data.workflow_id==WORKFLOW_ID]\n",
    "\n",
    "    # extract annotations\n",
    "    print('Extracting annotations')\n",
    "    extracted_data = extract_data(classification_data=classification_data)\n",
    "\n",
    "    # aggregate annotations\n",
    "    print('Aggregating data')\n",
    "    final_data = aggregate_data(extracted_data=extracted_data)\n",
    "\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0291a-d5e7-4fca-88d0-6930d32f2d4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
